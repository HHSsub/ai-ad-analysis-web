#!/usr/bin/env python3
"""
YouTube ê´‘ê³  ë™ì˜ìƒ URL ìë™ ìˆ˜ì§‘ ì—”ì§„ (DB ì—°ë™ ë²„ì „)
- ì¤‘ë³µ í˜¸ì¶œ ë°©ì§€
- ë°ì´í„°ë² ì´ìŠ¤ ëˆ„ì  ì €ì¥
- ì›¹ì„œë¹„ìŠ¤ ì—°ë™ ì¤€ë¹„
"""

import requests
import time
import json
import os
from typing import List, Dict, Optional
from dataclasses import dataclass
import logging
from datetime import datetime

# ë¡œì»¬ DB ëª¨ë“ˆ import
try:
    from database_setup import YouTubeAdsDatabase
except ImportError:
    print("âŒ database_setup.py íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤!")
    exit(1)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class AdVideoInfo:
    """ê´‘ê³  ë¹„ë””ì˜¤ ì •ë³´ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    title: str
    url: str
    note: str

class YouTubeAdsCollectorDB:
    """YouTube ê´‘ê³  ë™ì˜ìƒ URL ìˆ˜ì§‘ê¸° (DB ì—°ë™ ë²„ì „)"""
    
    def __init__(self, apify_token: Optional[str] = None, serp_api_key: Optional[str] = None, db_path: str = "youtube_ads.db"):
        self.apify_token = apify_token or os.getenv('APIFY_TOKEN')
        self.serp_api_key = serp_api_key or os.getenv('SERPAPI_KEY')
        self.db = YouTubeAdsDatabase(db_path)
        
    def collect_ads_with_apify(self, search_query: str, max_ads: int = 50) -> List[AdVideoInfo]:
        """Apify YouTube Ads Scraperë¥¼ ì‚¬ìš©í•œ ê´‘ê³  ìˆ˜ì§‘"""
        if not self.apify_token:
            logger.error("Apify tokenì´ í•„ìš”í•©ë‹ˆë‹¤.")
            return []
            
        # ğŸ”¥ ì¤‘ë³µ í˜¸ì¶œ ë°©ì§€ ì²´í¬
        if not self.db.should_collect(search_query, "Apify", hours=24):
            logger.info(f"â­ï¸ Apify '{search_query}' ìˆ˜ì§‘ ê±´ë„ˆë›°ê¸° (24ì‹œê°„ ì´ë‚´ ìˆ˜ì§‘ë¨)")
            return []
        
        url = "https://api.apify.com/v2/acts/xtech~youtube-ads-scraper/run-sync-get-dataset-items"
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.apify_token}"
        }
        data = {"max_ads": max_ads}
        
        try:
            logger.info(f"ğŸ“¡ Apifyë¡œ '{search_query}' ìˆ˜ì§‘ ì¤‘...")
            response = requests.post(url, headers=headers, json=data, timeout=300)
            response.raise_for_status()
            
            ads_data = response.json()
            logger.info(f"   ğŸ“¥ ìˆ˜ì‹ ëœ ë°ì´í„°: {len(ads_data)}ê°œ")
            
            ad_videos = []
            for ad in ads_data:
                if isinstance(ad, dict):
                    video_id = ad.get('video_id', '')
                    youtube_url = f"https://www.youtube.com/watch?v={video_id}" if video_id else ""
                    
                    title = ""
                    if 'youtubeData' in ad and 'title' in ad['youtubeData']:
                        title = ad['youtubeData']['title'].strip()
                    
                    note_parts = [f"âœ… Apify í™•ì‹¤í•œ ê´‘ê³ "]
                    if 'advertiser_id' in ad:
                        note_parts.append(f"ê´‘ê³ ì£¼ID: {ad['advertiser_id']}")
                    if 'youtubeStatistics' in ad:
                        stats = ad['youtubeStatistics']
                        if 'viewCount' in stats:
                            note_parts.append(f"ì¡°íšŒìˆ˜: {stats['viewCount']}")
                    
                    note = " | ".join(note_parts)
                    
                    if youtube_url and title:
                        ad_video = AdVideoInfo(
                            title=title[:150],
                            url=youtube_url,
                            note=note[:200]
                        )
                        ad_videos.append(ad_video)
            
            logger.info(f"   âœ… ì²˜ë¦¬ëœ ê´‘ê³ : {len(ad_videos)}ê°œ")
            return ad_videos
            
        except requests.exceptions.RequestException as e:
            logger.error(f"Apify API ìš”ì²­ ì‹¤íŒ¨: {e}")
            return []
        except Exception as e:
            logger.error(f"Apify ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return []
    
    def collect_ads_with_serpapi(self, search_query: str) -> List[AdVideoInfo]:
        """SerpAPIë¥¼ ì‚¬ìš©í•œ YouTube ê´‘ê³  ê²€ìƒ‰"""
        if not self.serp_api_key:
            logger.error("SerpAPI í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            return []
        
        # ğŸ”¥ ì¤‘ë³µ í˜¸ì¶œ ë°©ì§€ ì²´í¬
        if not self.db.should_collect(search_query, "SerpAPI", hours=6):  # SerpAPIëŠ” 6ì‹œê°„
            logger.info(f"â­ï¸ SerpAPI '{search_query}' ìˆ˜ì§‘ ê±´ë„ˆë›°ê¸° (6ì‹œê°„ ì´ë‚´ ìˆ˜ì§‘ë¨)")
            return []
        
        url = "https://serpapi.com/search"
        params = {
            "engine": "youtube",
            "search_query": search_query,
            "api_key": self.serp_api_key,
            "num": 20
        }
        
        try:
            logger.info(f"ğŸ“¡ SerpAPIë¡œ '{search_query}' ìˆ˜ì§‘ ì¤‘...")
            response = requests.get(url, params=params, timeout=60)
            
            if response.status_code == 200:
                data = response.json()
                
                if "error" in data:
                    logger.error(f"SerpAPI ì˜¤ë¥˜: {data['error']}")
                    return []
                
                ad_videos = []
                
                # ì‹¤ì œ ê´‘ê³  ê²°ê³¼ ì²˜ë¦¬
                ads_results = data.get("ads_results", [])
                for ad in ads_results:
                    title = ad.get('title', 'Unknown Title').strip()
                    link = ad.get('link', '')
                    
                    if link and 'youtube.com' in link:
                        note_parts = [f"ğŸ“¢ SerpAPI ê´‘ê³ "]
                        if 'views' in ad:
                            note_parts.append(f"ì¡°íšŒìˆ˜: {ad['views']}")
                        if 'channel' in ad and 'name' in ad['channel']:
                            note_parts.append(f"ì±„ë„: {ad['channel']['name']}")
                        
                        note = " | ".join(note_parts)
                        
                        ad_video = AdVideoInfo(
                            title=title[:150],
                            url=link,
                            note=note[:200]
                        )
                        ad_videos.append(ad_video)
                
                # ê´‘ê³ ì„± í‚¤ì›Œë“œ ë¹„ë””ì˜¤ í•„í„°ë§
                video_results = data.get("video_results", [])
                ad_keywords = ['ad', 'advertisement', 'commercial', 'sponsored', 'promo', 'review', 'unboxing']
                
                for video in video_results:
                    title = video.get('title', '').strip()
                    link = video.get('link', '')
                    
                    if any(keyword in title.lower() for keyword in ad_keywords):
                        if link and 'youtube.com' in link:
                            note_parts = [f"ğŸ¬ SerpAPI ê´‘ê³ ì„± ì½˜í…ì¸ "]
                            if 'views' in video:
                                note_parts.append(f"ì¡°íšŒìˆ˜: {video['views']}")
                            if 'channel' in video and 'name' in video['channel']:
                                note_parts.append(f"ì±„ë„: {video['channel']['name']}")
                            
                            note = " | ".join(note_parts)
                            
                            ad_video = AdVideoInfo(
                                title=title[:150],
                                url=link,
                                note=note[:200]
                            )
                            ad_videos.append(ad_video)
                
                logger.info(f"   âœ… ìˆ˜ì§‘ëœ ê´‘ê³ : {len(ad_videos)}ê°œ")
                return ad_videos
                
            else:
                logger.error(f"SerpAPI ìš”ì²­ ì‹¤íŒ¨: HTTP {response.status_code}")
                return []
                
        except requests.exceptions.RequestException as e:
            logger.error(f"SerpAPI ìš”ì²­ ì‹¤íŒ¨: {e}")
            return []
        except Exception as e:
            logger.error(f"SerpAPI ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return []
    
    def collect_all_ads(self, search_queries: List[str] = None, max_ads_per_query: int = 30) -> Dict[str, int]:
        """
        ëª¨ë“  ë°©ë²•ìœ¼ë¡œ ê´‘ê³  ìˆ˜ì§‘ ë° DB ì €ì¥
        
        Returns:
            {'total_collected': 50, 'new_ads': 25, 'apify': 20, 'serpapi': 5}
        """
        if search_queries is None:
            search_queries = [
                "advertisement commercial",
                "product promotion",
                "brand commercial",
                "sponsored content",
                "new product launch",
                "company ad",
                "marketing video",
                "product review"
            ]
        
        results = {
            'total_collected': 0,
            'new_ads': 0,
            'apify': 0,
            'serpapi': 0,
            'skipped_queries': 0
        }
        
        logger.info(f"ğŸš€ ê´‘ê³  ìˆ˜ì§‘ ì‹œì‘ - {len(search_queries)}ê°œ ê²€ìƒ‰ì–´")
        
        for i, query in enumerate(search_queries, 1):
            print(f"\nğŸ“ [{i}/{len(search_queries)}] ê²€ìƒ‰ì–´: '{query}'")
            
            collected_this_query = 0
            
            # Apify ìˆ˜ì§‘
            if self.apify_token:
                apify_ads = self.collect_ads_with_apify(query, max_ads_per_query)
                if apify_ads:
                    new_count = self.db.save_ads(apify_ads, query, "Apify")
                    results['total_collected'] += len(apify_ads)
                    results['new_ads'] += new_count
                    results['apify'] += len(apify_ads)
                    collected_this_query += len(apify_ads)
                    time.sleep(2)  # API ìš”ì²­ ê°„ê²©
            
            # SerpAPI ìˆ˜ì§‘
            if self.serp_api_key:
                serpapi_ads = self.collect_ads_with_serpapi(query)
                if serpapi_ads:
                    new_count = self.db.save_ads(serpapi_ads, query, "SerpAPI")
                    results['total_collected'] += len(serpapi_ads)
                    results['new_ads'] += new_count
                    results['serpapi'] += len(serpapi_ads)
                    collected_this_query += len(serpapi_ads)
                    time.sleep(1)  # API ìš”ì²­ ê°„ê²©
            
            if collected_this_query == 0:
                results['skipped_queries'] += 1
                print(f"   â­ï¸ ê±´ë„ˆë›°ê¸° (ìµœê·¼ ìˆ˜ì§‘ë¨ ë˜ëŠ” ì˜¤ë¥˜)")
            else:
                print(f"   âœ… ì´ë²ˆ ì¿¼ë¦¬ ìˆ˜ì§‘: {collected_this_query}ê°œ")
        
        return results
    
    def get_database_stats(self) -> dict:
        """ë°ì´í„°ë² ì´ìŠ¤ í†µê³„ ì¡°íšŒ"""
        return self.db.get_statistics()
    
    def export_for_web_service(self, status: str = 'pending', limit: int = 100) -> list:
        """
        ì›¹ì„œë¹„ìŠ¤ ì—°ë™ìš© ë°ì´í„° ì¶”ì¶œ
        
        Args:
            status: 'pending', 'all'
            limit: ìµœëŒ€ ê°œìˆ˜
            
        Returns:
            [{'id': 1, 'title': '...', 'url': '...', 'note': '...'}, ...]
        """
        if status == 'pending':
            return self.db.get_pending_analysis(limit)
        else:
            # ëª¨ë“  ë°ì´í„° ì¡°íšŒ ë¡œì§ (í•„ìš”ì‹œ êµ¬í˜„)
            pass

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ YouTube ê´‘ê³  ë™ì˜ìƒ URL ìë™ ìˆ˜ì§‘ ì—”ì§„ (DB ì—°ë™)")
    print("=" * 60)
    
    # API í‚¤ ì„¤ì •
    apify_token = os.getenv('APIFY_TOKEN')
    serp_api_key = os.getenv('SERPAPI_KEY')
    
    if not apify_token and not serp_api_key:
        print("ğŸ”‘ API í‚¤ ì…ë ¥")
        serp_api_key = "646e6386e54a3e331122aa9460166830bcdbd35c89283b857dcf66901e11db2a"
        if not apify_token and not serp_api_key:
            logger.error("ìµœì†Œ í•˜ë‚˜ì˜ API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤!")
            return
    
    # ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”
    collector = YouTubeAdsCollectorDB(
        apify_token=apify_token,
        serp_api_key=serp_api_key
    )
    
    # í˜„ì¬ DB ìƒíƒœ ì¶œë ¥
    stats = collector.get_database_stats()
    print(f"\nğŸ“Š í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ:")
    print(f"   ì „ì²´ ê´‘ê³ : {stats['total_ads']}ê°œ")
    print(f"   ë¶„ì„ ëŒ€ê¸°: {stats['pending']}ê°œ")
    print(f"   ë¶„ì„ ì™„ë£Œ: {stats['completed']}ê°œ")
    print(f"   ìµœê·¼ ìˆ˜ì§‘: {stats.get('latest_collection', 'ì—†ìŒ')}")
    
    # ì‚¬ìš©ì ì„¤ì •
    try:
        max_ads = int(input(f"\nğŸ¯ ê²€ìƒ‰ì–´ë‹¹ ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜ (ê¸°ë³¸ê°’: 20): ") or "20")
        
        # ìˆ˜ì§‘ ì‹¤í–‰
        print(f"\nğŸš€ ê´‘ê³  ìˆ˜ì§‘ ì‹œì‘...")
        results = collector.collect_all_ads(max_ads_per_query=max_ads)
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"\n" + "="*60)
        print(f"ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½")
        print(f"="*60)
        print(f"âœ… ì´ ìˆ˜ì§‘: {results['total_collected']}ê°œ")
        print(f"ğŸ†• ì‹ ê·œ ê´‘ê³ : {results['new_ads']}ê°œ")
        print(f"ğŸ“¡ Apify: {results['apify']}ê°œ")
        print(f"ğŸ” SerpAPI: {results['serpapi']}ê°œ")
        print(f"â­ï¸ ê±´ë„ˆë›´ ê²€ìƒ‰ì–´: {results['skipped_queries']}ê°œ")
        
        # ì—…ë°ì´íŠ¸ëœ DB ìƒíƒœ
        final_stats = collector.get_database_stats()
        print(f"\nğŸ“ˆ ì—…ë°ì´íŠ¸ëœ DB ìƒíƒœ:")
        print(f"   ì „ì²´ ê´‘ê³ : {final_stats['total_ads']}ê°œ")
        print(f"   ë¶„ì„ ëŒ€ê¸°: {final_stats['pending']}ê°œ")
        
        if results['new_ads'] > 0:
            print(f"\nğŸ‰ ìˆ˜ì§‘ ì™„ë£Œ! {results['new_ads']}ê°œ ì‹ ê·œ ê´‘ê³ ê°€ DBì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            # ì›¹ì„œë¹„ìŠ¤ ì—°ë™ìš© ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
            pending_ads = collector.export_for_web_service('pending', 3)
            if pending_ads:
                print(f"\nğŸ“‹ ì›¹ì„œë¹„ìŠ¤ ì—°ë™ ëŒ€ê¸° ëª©ë¡ (ì²˜ìŒ 3ê°œ):")
                print(f"-" * 60)
                for i, ad in enumerate(pending_ads, 1):
                    print(f"{i}. ì œëª©: {ad['title'][:50]}...")
                    print(f"   URL: {ad['url']}")
                    print(f"   ìˆ˜ì§‘ì¼: {ad['collected_at']}")
                    print()
        else:
            print(f"\nğŸ’¡ ì‹ ê·œ ê´‘ê³ ê°€ ì—†ìŠµë‹ˆë‹¤. (ì¤‘ë³µ ì œê±°ë¨)")
            
    except KeyboardInterrupt:
        print("\n\nâ¸ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except ValueError:
        print("âŒ ì˜¬ë°”ë¥¸ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    except Exception as e:
        logger.error(f"ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":

    main()

